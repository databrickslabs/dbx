# http://yaml.org/spec/1.2/spec.html
# https://learnxinyminutes.com/docs/yaml/

custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "7.3.x-cpu-ml-scala2.12"
    node_type_id: "some-node-type"
    aws_attributes:
      first_on_demand: 0
      availability: "SPOT"
  basic-auto-scale-props: &basic-auto-scale-props
    autoscale:
      min_workers: 2
      max_workers: 5

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 2

  basic-autoscale-cluster: &basic-autoscale-cluster
    new_cluster:
      <<: # merge these two maps and place them here.
        - *basic-cluster-props
        - *basic-auto-scale-props

  basic-cluster-libraries: &basic-cluster-libraries
    libraries:
      - pypi:
          package: "pydash"


environments:
  default:
    jobs:
      - name: "your-job-name"
        <<: *basic-static-cluster
        libraries: []
        max_retries: 0
        spark_python_task:
          python_file: "tests/deployment-configs/placeholder_1.py"

      - name: "your-job-name-2"
        <<: *basic-static-cluster
        libraries: []
        max_retries: 0
        spark_python_task:
          python_file: "tests/deployment-configs/placeholder_2.py"

      - name: "your-job-name-3"
        <<:
          - *basic-static-cluster
          - *basic-cluster-libraries
        max_retries: 5
        spark_python_task:
          python_file: "tests/deployment-configs/placeholder_2.py"

      - name: "your-job-name-4"
        <<:
          - *basic-autoscale-cluster
          - *basic-cluster-libraries
        max_retries: 5
        spark_python_task:
          python_file: "tests/deployment-configs/placeholder_2.py"

custom:
  basic-cluster-props: &basic-cluster-props
    spark_version: "9.1.x-cpu-ml-scala2.12"

  basic-static-cluster: &basic-static-cluster
    new_cluster:
      <<: *basic-cluster-props
      num_workers: 1
      node_type_id: "{some-node-type-id}"

environments:
  default:
    strict_path_adjustment_policy: true
    jobs:
      - name: "dbx_jobs_v21_test"
        job_clusters:
          - job_cluster_key: "basic-cluster"
            <<: *basic-static-cluster
        tasks:
          - task_key: "first-task"
            job_cluster_key: "basic-cluster"
            spark_python_task:
              python_file: "file://some/entrypoint.py"
              parameters: ["--conf-file", "file:fuse://some/conf/file.yml"]
          - task_key: "second-task"
            job_cluster_key: "basic-cluster"
            spark_python_task:
              python_file: "file://some/entrypoint.py"
              parameters: ["--conf-file", "file:fuse://some/conf/file.yml"]
              depends_on:
              - task_key: "first-task"
        permissions:
          access_control_list: ## acl list needs to be exhaustive
            - user_name: "some_user@example.com"
              permission_level: "IS_OWNER"
            - group_name: "some-user-group"
              permission_level: "CAN_VIEW"
        ## add notifications of job activities
        email_notifications:
          on_start:
            - "some_user@example.com"
          on_success: 
            - "some_user@example.com"
          on_failure: 
            - "some_user@example.com"
            - "some_group@example.com"
          no_alert_for_skipped_runs: false
        ## add job schedule
        schedule:
          quartz_cron_expression: "16 30 * * * ?"
          timezone_id: "America/Chicago"
          pause_status: "PAUSED"
